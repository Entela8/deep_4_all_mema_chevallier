{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP4 — Phase 3 : Génération du Dataset via API Infomaniak\n",
    "\n",
    "**Tâche** : Analyse littéraire de paroles de musique  \n",
    "**Dataset source** : `brunokreiner/genius-lyrics`  \n",
    "**Teacher** : `openai/gpt-oss-120b` via API Infomaniak  \n",
    "**Deux stages** :\n",
    "- Stage 1 : température basse (τ=0.3) → réponses stables\n",
    "- Stage 2 : température haute (τ=0.9) → réponses diversifiées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des dépendances (décommenter si besoin sur Colab)\n",
    "# !pip install openai datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "import openai\n",
    "\n",
    "print('Imports OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY      = \"nKuJabWS1epvq3x-m8by6NOU4xP4_znNL9OhmgXBPz9OeWOHlyGJIENnG8oXLT-4oOXNmESqExEMZv6o\"\n",
    "BASE_URL     = \"https://api.infomaniak.com/2/ai/48/openai/v1\"\n",
    "TEACHER_MODEL = \"openai/gpt-oss-120b\"\n",
    "\n",
    "N_STAGE1   = 150   # Basse température\n",
    "N_STAGE2   = 150   # Haute température\n",
    "TEMP_STAGE1 = 0.3\n",
    "TEMP_STAGE2 = 0.9\n",
    "\n",
    "MIN_LYRICS_LEN = 300\n",
    "MAX_LYRICS_LEN = 2000\n",
    "\n",
    "OUTPUT_DIR = Path(\"data\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Output : {OUTPUT_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Prompt — Analyse littéraire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are an expert literary critic specializing in popular music.\n",
    "When given song lyrics, you must reason through the analysis carefully step by step.\n",
    "\n",
    "Always structure your response as follows:\n",
    "1. First, reason inside <reasoning>...</reasoning> tags where you:\n",
    "   - Identify the main themes and motifs\n",
    "   - Analyze the poetic devices (metaphors, repetition, rhyme scheme, imagery)\n",
    "   - Examine the emotional arc and narrative structure\n",
    "   - Consider the cultural and artistic context\n",
    "2. Then provide a concise, well-structured literary analysis as your final answer.\n",
    "\n",
    "Be thorough in your reasoning but clear and insightful in your final analysis.\"\"\"\n",
    "\n",
    "print(SYSTEM_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement et filtrage du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lyrics_samples(n_total: int, seed: int = 42) -> list:\n",
    "    \"\"\"Charge et filtre des paroles de qualité depuis le dataset.\"\"\"\n",
    "    print(\"Chargement du dataset genius-lyrics...\")\n",
    "    ds = load_dataset(\"brunokreiner/genius-lyrics\", split=\"train\")\n",
    "\n",
    "    filtered = []\n",
    "    for ex in ds:\n",
    "        lyrics = ex.get(\"lyrics\", \"\") or \"\"\n",
    "        if (\n",
    "            ex.get(\"is_english\", False)\n",
    "            and MIN_LYRICS_LEN <= len(lyrics) <= MAX_LYRICS_LEN\n",
    "            and len(lyrics.split()) >= 50\n",
    "        ):\n",
    "            filtered.append({\n",
    "                \"lyrics\":      lyrics,\n",
    "                \"artist_name\": ex.get(\"artist_name\") or \"Unknown Artist\",\n",
    "                \"genres\":      ex.get(\"genres_list\") or [],\n",
    "            })\n",
    "\n",
    "    print(f\"Exemples éligibles : {len(filtered)}\")\n",
    "    random.seed(seed)\n",
    "    return random.sample(filtered, min(n_total, len(filtered)))\n",
    "\n",
    "\n",
    "all_samples = load_lyrics_samples(N_STAGE1 + N_STAGE2)\n",
    "samples_stage1 = all_samples[:N_STAGE1]\n",
    "samples_stage2 = all_samples[N_STAGE1:]\n",
    "print(f\"Stage 1 : {len(samples_stage1)} exemples | Stage 2 : {len(samples_stage2)} exemples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions d'appel API Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(base_url=BASE_URL, api_key=API_KEY)\n",
    "\n",
    "def build_user_prompt(artist: str, lyrics: str) -> str:\n",
    "    artist_info = f\" by {artist}\" if artist != \"Unknown Artist\" else \"\"\n",
    "    return (\n",
    "        f\"Please provide a literary analysis of the following song lyrics{artist_info}:\\n\\n\"\n",
    "        f\"---\\n{lyrics}\\n---\\n\\n\"\n",
    "        \"Analyze the themes, poetic devices, emotional arc, and narrative structure.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def call_teacher_api(user_prompt: str, temperature: float, max_retries: int = 3) -> dict | None:\n",
    "    \"\"\"Appelle l'API et retourne contenu + logprobs. Retourne None en cas d'échec.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=TEACHER_MODEL,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\",   \"content\": user_prompt},\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                max_tokens=2000,\n",
    "                logprobs=True,\n",
    "                top_logprobs=1,\n",
    "            )\n",
    "            choice = response.choices[0]\n",
    "            logprobs_data = []\n",
    "            if choice.logprobs and choice.logprobs.content:\n",
    "                for t in choice.logprobs.content:\n",
    "                    logprobs_data.append({\"token\": t.token, \"logprob\": t.logprob})\n",
    "            return {\n",
    "                \"content\":       choice.message.content,\n",
    "                \"logprobs\":      logprobs_data,\n",
    "                \"finish_reason\": choice.finish_reason,\n",
    "            }\n",
    "        except openai.RateLimitError:\n",
    "            wait = 30 * (attempt + 1)\n",
    "            print(f\"  Rate limit. Attente {wait}s...\")\n",
    "            time.sleep(wait)\n",
    "        except Exception as e:\n",
    "            print(f\"  Erreur (tentative {attempt+1}/{max_retries}): {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(5)\n",
    "    return None\n",
    "\n",
    "\n",
    "print(\"Fonctions API définies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération Stage 1 — Température basse (τ = 0.3)\n",
    "\n",
    "Réponses stables et précises. Utilisées pour ancrer le raisonnement du student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stage(samples: list, stage: int, temperature: float) -> list:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"STAGE {stage} — température={temperature} — {len(samples)} exemples\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    results = []\n",
    "    llamafactory_data = []\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        print(f\"[{i+1}/{len(samples)}] {sample['artist_name']}\")\n",
    "        user_prompt = build_user_prompt(sample[\"artist_name\"], sample[\"lyrics\"])\n",
    "        api_result  = call_teacher_api(user_prompt, temperature)\n",
    "\n",
    "        if api_result is None or len(api_result[\"content\"]) < 200:\n",
    "            print(\"  → Ignoré\")\n",
    "            continue\n",
    "\n",
    "        results.append({\n",
    "            \"stage\":       stage,\n",
    "            \"temperature\": temperature,\n",
    "            \"artist_name\": sample[\"artist_name\"],\n",
    "            \"genres\":      sample[\"genres\"],\n",
    "            \"lyrics\":      sample[\"lyrics\"],\n",
    "            \"instruction\": user_prompt,\n",
    "            \"response\":    api_result[\"content\"],\n",
    "            \"logprobs\":    api_result[\"logprobs\"],\n",
    "            \"finish_reason\": api_result[\"finish_reason\"],\n",
    "        })\n",
    "        llamafactory_data.append({\n",
    "            \"conversations\": [\n",
    "                {\"from\": \"system\", \"value\": SYSTEM_PROMPT},\n",
    "                {\"from\": \"human\",  \"value\": user_prompt},\n",
    "                {\"from\": \"gpt\",    \"value\": api_result[\"content\"]},\n",
    "            ]\n",
    "        })\n",
    "        print(f\"  → OK ({len(api_result['content'])} chars, {len(api_result['logprobs'])} tokens)\")\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Sauvegarde\n",
    "    raw_file = OUTPUT_DIR / f\"stage{stage}_raw.json\"\n",
    "    lmf_file = OUTPUT_DIR / f\"stage{stage}_llamafactory.json\"\n",
    "    with open(raw_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    with open(lmf_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(llamafactory_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"\\n✓ {len(results)} exemples sauvegardés → {raw_file}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "stage1_results = generate_stage(samples_stage1, stage=1, temperature=TEMP_STAGE1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération Stage 2 — Température haute (τ = 0.9)\n",
    "\n",
    "Réponses plus créatives et diversifiées. Enrichissent la distribution d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_results = generate_stage(samples_stage2, stage=2, temperature=TEMP_STAGE2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récapitulatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"RÉCAPITULATIF GÉNÉRATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Stage 1 (τ={TEMP_STAGE1}) : {len(stage1_results)} exemples\")\n",
    "print(f\"Stage 2 (τ={TEMP_STAGE2}) : {len(stage2_results)} exemples\")\n",
    "print(f\"Total             : {len(stage1_results) + len(stage2_results)} exemples\")\n",
    "print(f\"Fichiers dans     : {OUTPUT_DIR.resolve()}\")\n",
    "print()\n",
    "print(\"Prochaine étape : Phase 4 — DAS filtering (sur Colab avec GPU)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
