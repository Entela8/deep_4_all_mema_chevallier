{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP4 — Phase 4 : Filtrage DAS (Divergence-Aware Sampling)\n",
    "\n",
    "> **⚠️ Ce notebook nécessite un GPU** — À exécuter sur Google Colab (T4) ou Kaggle (2×T4)\n",
    "\n",
    "**Objectif** : Filtrer les exemples générés en Phase 3 selon leur valeur pédagogique.  \n",
    "**Principe** : On conserve les exemples où le Teacher est confiant mais le Student hésite (*Teacher Sentences*).\n",
    "\n",
    "| Type | Condition | Action |\n",
    "|------|-----------|--------|\n",
    "| Teacher Sentence | P_teacher >> P_student | **GARDER** |\n",
    "| Shared Sentence  | P_teacher ≈ P_student  | garder |\n",
    "| Student Sentence | P_student > P_teacher  | **REJETER** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate bitsandbytes openai matplotlib\n",
    "print(\"Installation terminée.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "print(f\"PyTorch : {torch.__version__}\")\n",
    "print(f\"GPU disponible : {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU : {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload des données (depuis Phase 3)\n",
    "\n",
    "Uploadez les fichiers `stage1_raw.json` et `stage2_raw.json` générés localement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "from pathlib import Path\n",
    "\n",
    "# ── Option A : Upload depuis votre machine ──────────────────────────────\n",
    "# Cliquez sur \"Choisir des fichiers\" et sélectionnez :\n",
    "#   stage1_raw.json   ET   stage2_raw.json\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Vérification : les fichiers atterrissent dans /content/\n",
    "DATA_DIR   = Path(\"/content\")\n",
    "OUTPUT_DIR = Path(\"/content/data_filtered\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "for fname in [\"stage1_raw.json\", \"stage2_raw.json\"]:\n",
    "    fpath = DATA_DIR / fname\n",
    "    if fpath.exists():\n",
    "        import json\n",
    "        with open(fpath, encoding=\"utf-8\") as f:\n",
    "            n = len(json.load(f))\n",
    "        print(f\"  ✓ {fname} — {n} exemples\")\n",
    "    else:\n",
    "        print(f\"  ✗ {fname} — MANQUANT, re-uploadez ce fichier\")\n",
    "\n",
    "# ── Option B : Depuis Google Drive ──────────────────────────────────────\n",
    "# Décommenter les lignes suivantes si vous préférez Drive :\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# DATA_DIR = Path('/content/drive/MyDrive/tp4/data')\n",
    "\n",
    "print(f\"\\nSortie filtrée : {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classe DASPipelineQwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import time\n\nclass DASPipelineQwen:\n    def __init__(self, openai_api_key, student_model_id=\"unsloth/Qwen3-4B-Instruct-2507-unsloth-bnb-4bit\"):\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.float16,\n        )\n        self.student_model_id = student_model_id\n        print(f\"Chargement du modèle étudiant : {self.student_model_id}...\")\n\n        self.tokenizer = AutoTokenizer.from_pretrained(self.student_model_id, trust_remote_code=True)\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n        self.tokenizer.padding_side = \"left\"   # left-padding pour batch causal LM\n\n        self.model = AutoModelForCausalLM.from_pretrained(\n            self.student_model_id,\n            quantization_config=bnb_config,\n            device_map=\"auto\",\n            trust_remote_code=True,\n            torch_dtype=torch.float16,\n        )\n        self.model.eval()\n        print(\"Modèle étudiant chargé.\")\n\n        self.client = OpenAI(\n            api_key=openai_api_key,\n            base_url=\"https://api.infomaniak.com/2/ai/48/openai/v1\",\n        )\n        self.teacher_model_name = \"openai/gpt-oss-120b\"\n\n    # ----------------------------------------------------------------\n    # Forward pass en BATCH  (le cœur de l'optimisation)\n    # ----------------------------------------------------------------\n    def get_student_logprobs_batch(\n        self,\n        prompts: list,\n        responses: list,\n        max_length: int = 768,\n    ) -> list:\n        \"\"\"\n        Calcule les logprobs student pour N exemples en un seul forward pass.\n        ~N× plus rapide que N appels individuels.\n        max_length : longueur max en tokens (tronque les séquences longues).\n        \"\"\"\n        full_texts, prompt_lengths = [], []\n\n        for prompt, response in zip(prompts, responses):\n            # Texte complet prompt + réponse\n            full = self.tokenizer.apply_chat_template(\n                [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"assistant\", \"content\": response}],\n                tokenize=False,\n            )\n            # Longueur du prompt seul (pour masquer les labels)\n            prompt_only = self.tokenizer.apply_chat_template(\n                [{\"role\": \"user\", \"content\": prompt}],\n                tokenize=False, add_generation_prompt=True,\n            )\n            n_prompt = self.tokenizer(\n                prompt_only, return_tensors=\"pt\", add_special_tokens=False\n            ).input_ids.shape[1]\n\n            full_texts.append(full)\n            prompt_lengths.append(n_prompt)\n\n        # Tokenisation groupée avec padding + troncature\n        enc = self.tokenizer(\n            full_texts,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=max_length,\n        )\n        input_ids      = enc.input_ids.to(self.model.device)\n        attention_mask = enc.attention_mask.to(self.model.device)\n\n        # Labels : masquer le prompt et le padding\n        labels = input_ids.clone()\n        for i, p_len in enumerate(prompt_lengths):\n            labels[i, :p_len] = -100          # masque prompt\n        labels[attention_mask == 0] = -100    # masque padding\n\n        with torch.no_grad():\n            outputs      = self.model(input_ids, attention_mask=attention_mask)\n            shift_logits = outputs.logits[..., :-1, :].contiguous().float()\n            shift_labels = labels[..., 1:].contiguous()\n            loss_fct     = torch.nn.CrossEntropyLoss(reduction=\"none\", ignore_index=-100)\n            token_losses = loss_fct(shift_logits.transpose(1, 2), shift_labels)\n\n        results = []\n        for i in range(len(prompts)):\n            valid_mask    = shift_labels[i] != -100\n            valid_logprobs = (-token_losses[i][valid_mask]).cpu().numpy()\n            mean_lp = float(np.exp(np.mean(valid_logprobs))) if len(valid_logprobs) > 0 else 0.0\n            results.append({\"mean_logprob\": mean_lp, \"num_tokens\": len(valid_logprobs)})\n\n        return results\n\n    # ----------------------------------------------------------------\n    # Décision DAS\n    # ----------------------------------------------------------------\n    def decide_keep_prompt(self, teacher_mean_prob, student_mean_prob, threshold=0.1):\n        divergence = teacher_mean_prob - student_mean_prob\n        if divergence > threshold:\n            label, keep = \"TEACHER_SENTENCE\", True\n        elif divergence < -threshold:\n            label, keep = \"STUDENT_SENTENCE\", False\n        else:\n            label, keep = \"SHARED_SENTENCE\", True\n        return {\"keep\": keep, \"label\": label, \"divergence\": divergence,\n                \"teacher_prob\": teacher_mean_prob, \"student_prob\": student_mean_prob}\n\n    # ----------------------------------------------------------------\n    # Filtrage du dataset — version batched\n    # ----------------------------------------------------------------\n    def filter_dataset(self, input_path, output_dir, stage, threshold=0.1,\n                       batch_size=8, max_length=768):\n        \"\"\"\n        batch_size : exemples traités en parallèle (GPU).\n                     Augmenter si pas d'OOM (8→16→32).\n        max_length : tronque les séquences longues pour accélérer.\n        \"\"\"\n        output_dir = Path(output_dir)\n        output_dir.mkdir(parents=True, exist_ok=True)\n\n        with open(input_path, \"r\", encoding=\"utf-8\") as f:\n            examples = json.load(f)\n\n        n_total = len(examples)\n        print(f\"\\n{'='*60}\")\n        print(f\"FILTRAGE DAS — Stage {stage} | {n_total} ex | batch={batch_size} | max_len={max_length}\")\n        print(f\"{'='*60}\")\n\n        filtered_raw, llamafactory_data, all_scores = [], [], []\n        t_start = time.time()\n\n        # Traitement par batches\n        for b_start in range(0, n_total, batch_size):\n            batch = examples[b_start : b_start + batch_size]\n\n            prompts   = [ex[\"instruction\"] for ex in batch]\n            responses = [ex[\"response\"]    for ex in batch]\n\n            # Logprobs teacher (déjà dans le fichier Phase 3)\n            teacher_mean_probs = []\n            for ex in batch:\n                lps = [lp[\"logprob\"] for lp in ex.get(\"logprobs\", [])]\n                teacher_mean_probs.append(float(np.exp(np.mean(lps))) if lps else 0.0)\n\n            try:\n                # Un seul forward pass pour tout le batch\n                student_results = self.get_student_logprobs_batch(prompts, responses, max_length)\n            except Exception as e:\n                print(f\"\\n  ERREUR batch [{b_start}:{b_start+len(batch)}] : {e}\")\n                continue\n\n            for j, (ex, t_prob, s_res) in enumerate(zip(batch, teacher_mean_probs, student_results)):\n                decision = self.decide_keep_prompt(t_prob, s_res[\"mean_logprob\"], threshold)\n                all_scores.append(decision[\"divergence\"])\n\n                idx    = b_start + j + 1\n                artist = ex.get(\"artist_name\", \"Unknown\")[:25]\n                status = \"KEEP\" if decision[\"keep\"] else \"SKIP\"\n                print(f\"[{idx:>4}/{n_total}] {artist:<25} {status} | div={decision['divergence']:+.4f} | {decision['label']}\")\n\n                if decision[\"keep\"]:\n                    filtered_raw.append({**ex, **{f\"das_{k}\": v for k, v in decision.items()}})\n                    llamafactory_data.append({\n                        \"conversations\": [\n                            {\"from\": \"human\", \"value\": ex[\"instruction\"]},\n                            {\"from\": \"gpt\",   \"value\": ex[\"response\"]},\n                        ]\n                    })\n\n            # ETA après chaque batch\n            elapsed  = time.time() - t_start\n            done     = b_start + len(batch)\n            per_ex   = elapsed / done\n            remaining = (n_total - done) * per_ex\n            print(f\"  ↳ batch terminé — {elapsed:.0f}s écoulées — ETA {remaining:.0f}s ({remaining/60:.1f} min)\")\n\n        # Sauvegarde\n        raw_out  = output_dir / f\"stage{stage}_filtered_raw.json\"\n        lmf_out  = output_dir / f\"stage{stage}_filtered_llamafactory.json\"\n        plot_out = output_dir / f\"stage{stage}_das_scores.png\"\n\n        with open(raw_out,  \"w\", encoding=\"utf-8\") as f: json.dump(filtered_raw,      f, ensure_ascii=False, indent=2)\n        with open(lmf_out,  \"w\", encoding=\"utf-8\") as f: json.dump(llamafactory_data, f, ensure_ascii=False, indent=2)\n\n        if all_scores:\n            fig, ax = plt.subplots(figsize=(9, 5))\n            ax.hist(all_scores, bins=30, color=\"steelblue\", edgecolor=\"white\", alpha=0.85)\n            ax.axvline(x= threshold, color=\"green\",  linestyle=\"--\", linewidth=1.5, label=f\"Seuil KEEP (+{threshold})\")\n            ax.axvline(x=-threshold, color=\"red\",    linestyle=\"--\", linewidth=1.5, label=f\"Seuil REJECT (-{threshold})\")\n            ax.axvline(x=0,          color=\"orange\", linestyle=\":\",  linewidth=1.2, label=\"Divergence = 0\")\n            ax.set_xlabel(\"Divergence (P_teacher − P_student)\", fontsize=12)\n            ax.set_ylabel(\"Nombre d'exemples\", fontsize=12)\n            ax.set_title(f\"Distribution DAS — Stage {stage}\", fontsize=14)\n            ax.legend(); fig.tight_layout(); fig.savefig(plot_out, dpi=150); plt.show()\n\n        n_kept = len(filtered_raw)\n        total_time = time.time() - t_start\n        print(f\"\\n── Résultats Stage {stage} ──────────────────────────────────\")\n        print(f\"  Conservés      : {n_kept}/{n_total} ({100*n_kept/n_total:.1f}%)\")\n        print(f\"  Rejetés        : {n_total - n_kept}/{n_total}\")\n        print(f\"  Div. moyenne   : {float(np.mean(all_scores)):+.4f}\")\n        print(f\"  Temps total    : {total_time:.0f}s ({total_time/60:.1f} min)\")\n        print(f\"  Temps/exemple  : {total_time/n_total:.1f}s\")\n        return filtered_raw\n\nprint(\"Classe DASPipelineQwen (batched) définie.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chargement du pipeline (Student + Teacher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY    = \"nKuJabWS1epvq3x-m8by6NOU4xP4_znNL9OhmgXBPz9OeWOHlyGJIENnG8oXLT-4oOXNmESqExEMZv6o\"\n",
    "STUDENT_ID = \"unsloth/Qwen3-4B-Instruct-2507-unsloth-bnb-4bit\"\n",
    "\n",
    "pipeline = DASPipelineQwen(openai_api_key=API_KEY, student_model_id=STUDENT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Filtrage Stage 1 (τ = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage1_filtered = pipeline.filter_dataset(\n",
    "    input_path = DATA_DIR / \"stage1_raw.json\",\n",
    "    output_dir = OUTPUT_DIR,\n",
    "    stage      = 1,\n",
    "    threshold  = 0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Filtrage Stage 2 (τ = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_filtered = pipeline.filter_dataset(\n",
    "    input_path = DATA_DIR / \"stage2_raw.json\",\n",
    "    output_dir = OUTPUT_DIR,\n",
    "    stage      = 2,\n",
    "    threshold  = 0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Récapitulatif & Téléchargement des fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"RÉCAPITULATIF PHASE 4 — DAS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Stage 1 conservés : {len(stage1_filtered)}\")\n",
    "print(f\"Stage 2 conservés : {len(stage2_filtered)}\")\n",
    "print(f\"Total pour entraînement : {len(stage1_filtered) + len(stage2_filtered)}\")\n",
    "print()\n",
    "print(\"Fichiers prêts pour LLaMA-Factory :\")\n",
    "for f in sorted(OUTPUT_DIR.iterdir()):\n",
    "    print(f\"  {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Télécharger les fichiers filtrés sur votre machine\n",
    "from google.colab import files\n",
    "for f in OUTPUT_DIR.glob(\"*.json\"):\n",
    "    files.download(str(f))\n",
    "for f in OUTPUT_DIR.glob(\"*.png\"):\n",
    "    files.download(str(f))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}